{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7fb6cb-900f-4325-a6fe-decafafc3c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Projeto Final - Limpeza de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964497e2-4152-40e9-b5a7-84450252b2d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2854547a-e80f-4de4-ac03-d7d9e06d6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Modeling\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70a5d8b-d4bb-4a4b-9a54-25da8ffc838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a sessao do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"nyc_caio_garcia\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.4,com.microsoft.azure:azure-storage:8.6.6\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7d4876-0f97-4e38-a005-5d4023a78348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acesso aos dados na nuvem\n",
    "STORAGE_ACCOUNT = 'dlspadseastusprod'\n",
    "CONTAINER = 'big-data-comp-nuvem'\n",
    "FOLDER = 'airline-delay'\n",
    "TOKEN = 'lSuH4ZI9BhOFEhCF/7ZQbrpPBIhgtLcPDfXjJ8lMxQZjaADW4p6tcmiZGDX9u05o7FqSE2t9d2RD+ASt0YFG8g=='\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.\" + STORAGE_ACCOUNT + \".blob.core.windows.net\", TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c76c8-4ac8-4658-8f17-039cb3d4a8ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Schema\n",
    "Schema definido de acordo com o dicionário de dados em `projeto_final_dicionário.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7e09a0-9c71-4b06-a8ff-038b37d8b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "labels = (('FL_DATE', TimestampType()),\n",
    "          ('OP_CARRIER', StringType()),\n",
    "          ('OP_CARRIER_FL_NUM', IntegerType()),\n",
    "          ('ORIGIN', StringType()),\n",
    "          ('DEST', StringType()),\n",
    "          ('CRS_DEP_TIME', IntegerType()),\n",
    "          ('DEP_TIME', FloatType()),\n",
    "          ('DEP_DELAY', FloatType()),\n",
    "          ('TAXI_OUT', FloatType()),\n",
    "          ('WHEELS_OFF', FloatType()),\n",
    "          ('WHEELS_ON', FloatType()),\n",
    "          ('TAXI_IN', FloatType()),\n",
    "          ('CRS_ARR_TIME', IntegerType()),\n",
    "          ('ARR_TIME', FloatType()),\n",
    "          ('ARR_DELAY', FloatType()),\n",
    "          ('CANCELLED', FloatType()),\n",
    "          ('CANCELLATION_CODE', StringType()),\n",
    "          ('DIVERTED', FloatType()),\n",
    "          ('CRS_ELAPSED_TIME', FloatType()),\n",
    "          ('ACTUAL_ELAPSED_TIME', FloatType()),\n",
    "          ('AIR_TIME', FloatType()),\n",
    "          ('DISTANCE', FloatType()),\n",
    "          ('CARRIER_DELAY', FloatType()),\n",
    "          ('WEATHER_DELAY', FloatType()),\n",
    "          ('NAS_DELAY', FloatType()),\n",
    "          ('SECURITY_DELAY', FloatType()),\n",
    "          ('LATE_AIRCRAFT_DELAY', StringType()))\n",
    "\n",
    "schema = StructType([StructField(x[0], x[1], True) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff41ec0b-9ef9-4de4-b56d-0871910e2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with values in minutes\n",
    "minute_columns = [\"TAXI_OUT\",\"TAXI_IN\",\"DEP_DELAY\",\"ARR_DELAY\",\"AIR_TIME\",\"CRS_ELAPSED_TIME\",\"ACTUAL_ELAPSED_TIME\",\n",
    "                  \"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\"]\n",
    "\n",
    "# Subset from minute columns with no data leak from moment of take off\n",
    "clean_min_columns = [\"TAXI_OUT\", \"DEP_DELAY\", \"CRS_ELAPSED_TIME\"]\n",
    "\n",
    "# Columns with time information on format 'hhmm'\n",
    "# Not proper for numerical manipulation\n",
    "odd_format_columns = [\"CRS_DEP_TIME\",\"DEP_TIME\",\"WHEELS_OFF\",\"WHEELS_ON\",\"ARR_TIME\",\"CRS_ARR_TIME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4b10f-534e-4543-baf4-b157d802e015",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Carregamento de dados\n",
    "Dados carregados da nuvem como spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cbc885-775c-4991-ae70-8a2995b227e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(FL_DATE=datetime.datetime(2009, 1, 1, 0, 0), OP_CARRIER='XE', OP_CARRIER_FL_NUM=1204, ORIGIN='DCA', DEST='EWR', CRS_DEP_TIME=1100, DEP_TIME=1058.0, DEP_DELAY=-2.0, TAXI_OUT=18.0, WHEELS_OFF=1116.0, WHEELS_ON=1158.0, TAXI_IN=8.0, CRS_ARR_TIME=1202, ARR_TIME=1206.0, ARR_DELAY=4.0, CANCELLED=0.0, CANCELLATION_CODE=None, DIVERTED=0.0, CRS_ELAPSED_TIME=62.0, ACTUAL_ELAPSED_TIME=68.0, AIR_TIME=42.0, DISTANCE=199.0, CARRIER_DELAY=None, WEATHER_DELAY=None, NAS_DELAY=None, SECURITY_DELAY=None, LATE_AIRCRAFT_DELAY=None),\n",
       " Row(FL_DATE=datetime.datetime(2009, 1, 1, 0, 0), OP_CARRIER='XE', OP_CARRIER_FL_NUM=1206, ORIGIN='EWR', DEST='IAD', CRS_DEP_TIME=1510, DEP_TIME=1509.0, DEP_DELAY=-1.0, TAXI_OUT=28.0, WHEELS_OFF=1537.0, WHEELS_ON=1620.0, TAXI_IN=4.0, CRS_ARR_TIME=1632, ARR_TIME=1624.0, ARR_DELAY=-8.0, CANCELLED=0.0, CANCELLATION_CODE=None, DIVERTED=0.0, CRS_ELAPSED_TIME=82.0, ACTUAL_ELAPSED_TIME=75.0, AIR_TIME=43.0, DISTANCE=213.0, CARRIER_DELAY=None, WEATHER_DELAY=None, NAS_DELAY=None, SECURITY_DELAY=None, LATE_AIRCRAFT_DELAY=None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "config.set(\"fs.azure.account.key.\" + STORAGE_ACCOUNT + \".blob.core.windows.net\", TOKEN)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.csv(\"wasbs://{}@{}.blob.core.windows.net/{}/2009.csv\"\\\n",
    "                    .format(CONTAINER, STORAGE_ACCOUNT, FOLDER), header=True, schema=schema)\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3719918f-1480-47fa-a516-f60b7ff55b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVACOES = df.count()\n",
    "assert (OBSERVACOES == 6429338)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a63038-6084-4b33-be5d-e91df607d041",
   "metadata": {},
   "source": [
    "A base contem 6429338 observações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b5dc9f-3802-4a9a-bbfc-2a60220d732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCELAMENTOS = df.filter(df.CANCELLED == 1).count()\n",
    "assert (CANCELAMENTOS == 87038)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63098638-ce24-4cf9-88e3-5656cd3b0712",
   "metadata": {},
   "source": [
    "Dos voos na base, 87038 foram cancelados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3310590-09b3-4fed-be80-3d90b2a311d4",
   "metadata": {},
   "source": [
    "## Tratamento de dados faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de608229-774c-4f14-be45-7b48fdc1f298",
   "metadata": {},
   "source": [
    "Das 27 colunas na base de dados, 16 tem valores faltantes. Os dados ausentes na coluna CANCELATION_CODE sao 100% consistentes com a informacao de cancelamento, isto eh, apenas os voos que nao foram cancelados tem a coluna CANCELATION_CODE vazia. As outras 15 colunas podem ser agrupadas em 3 grupos: _Voo_, _Chegada_ e _Atrasos_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07a3be-4412-4013-9ee6-0f8abd3995e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d7d23b-f9ae-4234-adb2-ad02682b9c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_counts = df.select([sf.col(column).isNull().cast(\"int\").alias(column) for column in df.columns]) \\\n",
    "                       .groupBy() \\\n",
    "                       .sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7a7f14-7c6f-4abe-97bd-39286d92b574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sum(FL_DATE)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(OP_CARRIER)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(OP_CARRIER_FL_NUM)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(ORIGIN)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(DEST)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(CRS_DEP_TIME)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(DEP_TIME)</th>\n",
       "      <td>82867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(DEP_DELAY)</th>\n",
       "      <td>82867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(TAXI_OUT)</th>\n",
       "      <td>85787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(WHEELS_OFF)</th>\n",
       "      <td>85787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(WHEELS_ON)</th>\n",
       "      <td>89322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(TAXI_IN)</th>\n",
       "      <td>89321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(CRS_ARR_TIME)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(ARR_TIME)</th>\n",
       "      <td>89322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(ARR_DELAY)</th>\n",
       "      <td>102361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(CANCELLED)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(CANCELLATION_CODE)</th>\n",
       "      <td>6342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(DIVERTED)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(CRS_ELAPSED_TIME)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(ACTUAL_ELAPSED_TIME)</th>\n",
       "      <td>102362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(AIR_TIME)</th>\n",
       "      <td>102361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(DISTANCE)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(CARRIER_DELAY)</th>\n",
       "      <td>5258837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(WEATHER_DELAY)</th>\n",
       "      <td>5258837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(NAS_DELAY)</th>\n",
       "      <td>5258837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(SECURITY_DELAY)</th>\n",
       "      <td>5258837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum(LATE_AIRCRAFT_DELAY)</th>\n",
       "      <td>5258837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0\n",
       "sum(FL_DATE)                    0\n",
       "sum(OP_CARRIER)                 0\n",
       "sum(OP_CARRIER_FL_NUM)          0\n",
       "sum(ORIGIN)                     0\n",
       "sum(DEST)                       0\n",
       "sum(CRS_DEP_TIME)               0\n",
       "sum(DEP_TIME)               82867\n",
       "sum(DEP_DELAY)              82867\n",
       "sum(TAXI_OUT)               85787\n",
       "sum(WHEELS_OFF)             85787\n",
       "sum(WHEELS_ON)              89322\n",
       "sum(TAXI_IN)                89321\n",
       "sum(CRS_ARR_TIME)               0\n",
       "sum(ARR_TIME)               89322\n",
       "sum(ARR_DELAY)             102361\n",
       "sum(CANCELLED)                  0\n",
       "sum(CANCELLATION_CODE)    6342300\n",
       "sum(DIVERTED)                   0\n",
       "sum(CRS_ELAPSED_TIME)           0\n",
       "sum(ACTUAL_ELAPSED_TIME)   102362\n",
       "sum(AIR_TIME)              102361\n",
       "sum(DISTANCE)                   0\n",
       "sum(CARRIER_DELAY)        5258837\n",
       "sum(WEATHER_DELAY)        5258837\n",
       "sum(NAS_DELAY)            5258837\n",
       "sum(SECURITY_DELAY)       5258837\n",
       "sum(LATE_AIRCRAFT_DELAY)  5258837"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_counts.toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26e98d-433c-47c9-aec0-c7e4b57890af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cancelamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf163363-ebbf-4e68-a7b9-d50a267947ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.CANCELLATION_CODE.isNull()) &\n",
    "                  (df.CANCELLED == 0)).count() ==\n",
    "        df.filter(df.CANCELLED == 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c1b4a-9691-4607-a1ce-ce68c690819f",
   "metadata": {},
   "source": [
    "Todos os valores faltantes de CANCELLATION_CODE são referentes a voos que não foram cancelados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33922d-d997-47dd-a4df-fb7e779afa17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Voo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e02169-1998-49cd-8767-cf2530808c3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617e173-e81b-4654-81ef-c6abd3ed903f",
   "metadata": {},
   "source": [
    "`DEP_TIME` e `DEP_DELAY`: co-ausentes, todos cancelados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54caca1-c6fb-4108-bf45-62f0a8883da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.DEP_TIME.isNull())   &\n",
    "                  (df.DEP_DELAY.isNull())  &\n",
    "                  (df.TAXI_OUT.isNull())   &\n",
    "                  (df.WHEELS_OFF.isNull()) &\n",
    "                  (df.WHEELS_ON.isNull())  &\n",
    "                  (df.TAXI_IN.isNull())    &\n",
    "                  (df.ARR_TIME.isNull())).count() ==\n",
    "        df.filter(df.DEP_TIME.isNull()).count())\n",
    "\n",
    "assert (df.filter((df.DEP_TIME.isNull())   &\n",
    "                  (df.DEP_DELAY.isNull())).count() ==\n",
    "        df.filter(df.DEP_TIME.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "822e2907-17b5-4647-95ad-521095ccecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.DEP_TIME.isNull())   &\n",
    "                  (df.CANCELLED == 1)).count() ==\n",
    "        df.filter(df.DEP_TIME.isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b90782-bf9c-4e87-8dc4-78f5140b1024",
   "metadata": {},
   "source": [
    "`TAXI_OUT` e `WHEELS_OFF`: co-ausentes e cancelados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a4ec27-f674-4d71-879a-b8e23fa8ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_OUT.isNull())   &\n",
    "                  (df.WHEELS_OFF.isNull()) &\n",
    "                  (df.WHEELS_ON.isNull())  &\n",
    "                  (df.TAXI_IN.isNull())    &\n",
    "                  (df.ARR_TIME.isNull())).count() ==\n",
    "        df.filter(df.TAXI_OUT.isNull()).count())\n",
    "\n",
    "assert (df.filter((df.TAXI_OUT.isNull())   &\n",
    "                  (df.WHEELS_OFF.isNull())).count() ==\n",
    "        df.filter(df.TAXI_OUT.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd913d4-3138-4828-88a3-2096e267e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_OUT.isNull())   &\n",
    "                  (df.CANCELLED == 1)).count() ==\n",
    "        df.filter(df.TAXI_OUT.isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91137227-b6fe-47a3-81e5-901fe555a23e",
   "metadata": {},
   "source": [
    "`WHEELS_ON`, `TAXI_IN` e `ARR_TIME`: co-ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c143a583-4792-47d5-a1f2-73d4385092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.WHEELS_ON.isNull())  &\n",
    "                  (df.TAXI_IN.isNull())    &\n",
    "                  (df.ARR_TIME.isNull())).count() ==\n",
    "        df.filter(df.TAXI_IN.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d6f1ef6-a9d2-4be2-9470-ee291e9e95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_IN.isNull())   &\n",
    "                  (df.CANCELLED == 1)).count() ==\n",
    "        df.filter(df.CANCELLED == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05f1dd86-9259-4360-906c-b85e64847cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_IN.isNull()) &\n",
    "                  (df.CANCELLED == 0)   &\n",
    "                  (df.DIVERTED == 1)).count() ==\n",
    "        df.filter((df.TAXI_IN.isNull()) &\n",
    "                  (df.CANCELLED == 0)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d448802-ae82-49db-addf-6e876c8d0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_IN.isNull()) &\n",
    "                  (df.CANCELLED == 0)   &\n",
    "                  (df.DIVERTED == 1)).count() !=\n",
    "        df.filter(df.DIVERTED == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a800bab-9bd8-42c8-97bd-7cf31271b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_IN.isNotNull()) &\n",
    "                  (df.DIVERTED == 1)).count() == 13040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc45404-e314-42e5-8727-5c826ededb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.TAXI_IN.isNull()) &\n",
    "                  (df.DIVERTED == 1)).count() == 2283)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a7142-6bbe-48d3-a3e1-84113dce5562",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cedab5-fcbd-4a7e-aa6d-4dd72e0c3b23",
   "metadata": {},
   "source": [
    "O grupo _Voo_ apresenta uma relação entre voos cancelados e as 7 variáveis:\n",
    " - DEP_TIME\n",
    " - DEP_DELAY\n",
    " - TAXI_OUT\n",
    " - WHEELS_OFF\n",
    " - WHEELS_ON\n",
    " - TAXI_IN\n",
    " - ARR_TIME\n",
    " \n",
    "Os valores faltantes para WHEELS_ON, TAXI_IN e ARR_TIME coincidem nas mesmas observações (com uma exceção descrita mais abaixo). Todos os voos cancelados se encontram dentre essas observações. Os valores faltantes para TAXI_OUT e WHEELS_OFF coincidem nas mesmas observações, todas referentes a voos cancelados. Finalmente, os valores faltantes de DEP_TIME e DEP_DELAY coincidem nas mesmas observações, todas com valores faltantes para TAXI_OUT.\n",
    "\n",
    "Todos os voos que não foram cancelados mas não tem informação da hora de aterrisagem (`(df.WHEELS_ON.isNull()) & (df.CANCELLED == 0)`) foram redirecionados para um aeroporto diferente do aeroporto destino original (`df.DIVERTED == 1`)\n",
    "\n",
    "Destas relações, supomos:\n",
    " - A diferença entre DEP_TIME e WHEELS_OFF pode ser devido a voos que chegam a sair do chão antes de serem cancelados, e voos que são cancelados após o embarque mas antes da decolagem.\n",
    " - Nenhum desses valores faltantes parece implausível o suficiente para assumirmos erro nos dados baseado apenas nessa análise. Alguns desses dados podem vir a ser retirados mesmo assim por questão de propriedades dos algorítmos utilizados mais a frente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea660b-90ab-49cd-9b00-6d3c6ec62a2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chegada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898560b-3e2c-428b-b518-fbd51121f92a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Atrasos\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34976d73-0eba-47dc-b073-5e7617064c4b",
   "metadata": {},
   "source": [
    "#### Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67590453-4503-47e9-8b4d-a6050e5ee726",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.CANCELLED == 0) &\n",
    "                  (df.DEP_DELAY > 0)).count() == \n",
    "        2252608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "848dc97e-4dcc-48c3-b393-b68056591ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.CANCELLED == 0) &\n",
    "                  (df.ARR_DELAY > 0)).count() ==\n",
    "        2402990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "360b6632-a55e-4403-8b74-4b68cef59ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.CANCELLED == 0) &\n",
    "                  ((df.DEP_DELAY > 0) |\n",
    "                   (df.ARR_DELAY > 0))).count() ==\n",
    "        3052688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17d210a0-6792-4ca1-b80b-032fe3aeb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (OBSERVACOES - df.filter(df.CARRIER_DELAY.isNull()).count() ==\n",
    "        1170501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f63473d2-eb2c-46bb-81ea-3a16533465e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.CARRIER_DELAY.isNull()) &\n",
    "                  (df.WEATHER_DELAY.isNull()) &\n",
    "                  (df.NAS_DELAY.isNull()) &\n",
    "                  (df.SECURITY_DELAY.isNull()) &\n",
    "                  (df.LATE_AIRCRAFT_DELAY.isNull())).count() ==\n",
    "        df.filter(df.CARRIER_DELAY.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c6ac87d-a9e9-439b-83c3-e9638dd6adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert (df.filter((df.CARRIER_DELAY.isNull()) &\n",
    "#                   ((df.DEP_DELAY > 0) |\n",
    "#                    (df.ARR_DELAY > 0))).count() ==\n",
    "#         df.filter(df.CARRIER_DELAY.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdac41c-6b88-4273-90ed-28ce5ce51ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.CARRIER_DELAY == 0).select(df.OP_CARRIER_FL_NUM, df.CARRIER_DELAY).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d5baa-0389-4631-b701-4a6d89ef4e4f",
   "metadata": {},
   "source": [
    "#### Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fcc3b-629d-48c1-a863-6dc0208e31cf",
   "metadata": {},
   "source": [
    "Todos os dados faltantes referentes a categoria de atraso coincidem nas mesmas observações.\n",
    "\n",
    "Há menos observações com informação sobre a causa do atraso do que voos atrasados, independente se medindo o atraso de saída ou de chegada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b19f0e-bc57-4d53-9519-51dd48f1fc0e",
   "metadata": {},
   "source": [
    "### Anomalia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e21064-0258-41df-ba6c-06bd7d30b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.ACTUAL_ELAPSED_TIME.isNull()) &\n",
    "                  (df.AIR_TIME == 0)                &\n",
    "                  (df.WHEELS_ON.isNull())           &\n",
    "                  (df.ARR_TIME.isNull())            &\n",
    "                  (df.ARR_DELAY == 0)               &\n",
    "                  (df.TAXI_IN == 0)                 &\n",
    "                  (df.CANCELLED == 0)).count() == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1001ec0-a98d-4068-b913-da7feb48ba41",
   "metadata": {},
   "source": [
    "Uma mesma observação é responsavel pela discrepância na quantidade total de valores faltantes entre WHEELS_ON, TAXI_IN e ARR_TIME, e ARR_DELAY, ACTUAL_ELAPSED_TIME e AIR_TIME.\n",
    "Um valor de `AIR_TIME == 0` nao faz sentido para um voo que não foi cancelado, e o mesmo se aplica a `TAXI_IN == 0`. Ao retirar essa observação da base, a análise de dados faltantes por grupo torna-se mais consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ffb0f-0a6f-4fdd-a3eb-4f01eeaa7ffa",
   "metadata": {},
   "source": [
    "## Consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29821311-36a2-4c64-baa8-2aab9fe0b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter(df.AIR_TIME + df.TAXI_IN + df.TAXI_OUT != df.ACTUAL_ELAPSED_TIME).count() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581b7b1-f77f-4c6c-abb8-7f99a6d4557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.CANCELLED == 1) &\n",
    "                  (df.DIVERTED == 1)).count() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7d80-725a-4b26-a91d-e624ccac0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.filter((df.DEP_TIME % 1 != 0) | (df.DEP_DELAY % 1 != 0)).count() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9d7d4-c473-46ab-b19f-9f94255b7eb0",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0863c1-2799-4bd2-9bfd-74972e5cfaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list includes all values not known at the moment of takeoff\n",
    "# except `ARR_DELAY` which will be used as target variable\n",
    "take_off_leak = [\"WHEELS_ON\",\"TAXI_IN\",\"ARR_TIME\",\"ACTUAL_ELAPSED_TIME\",\"AIR_TIME\",\n",
    "                 \"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5b2c3c1-3354-4490-8614-d1609daea5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_off_df = df.drop(*take_off_leak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5360174-e35f-4751-bd02-388005e40f2b",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dba2f94e-d13e-4906-a646-616109db96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = take_off_df.randomSplit([0.8,0.2], seed=42)\n",
    "toy_df = train_df.sample(False, 0.01, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "812895dc-952d-41d1-ac37-399fb4ce6104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count: 5142441\n",
      "Test set count: 1286897\n",
      "Toy set count: 51765\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set count:\", train_df.count())\n",
    "print(\"Test set count:\", test_df.count())\n",
    "print(\"Toy set count:\", toy_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d1b45-2dff-4a86-baf8-807b738bfa30",
   "metadata": {},
   "source": [
    "## Feature Engineering: One-Hot-Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18399fb8-3041-4c43-b07e-cc785268ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"OP_CARRIER\", \"ORIGIN\", \"DEST\"] # \"OP_CARRIER_FL_NUM\",\n",
    "\n",
    "indexOutputCols = [x + 'Index' for x in cat_features]\n",
    "\n",
    "oheOutputCols = [x + 'OHE' for x in cat_features]\n",
    "\n",
    "stringIndex = StringIndexer(inputCols = cat_features,\n",
    "                            outputCols = indexOutputCols,\n",
    "                            handleInvalid = 'skip')\n",
    "\n",
    "oheEncoder = OneHotEncoder(inputCols = indexOutputCols,\n",
    "                           outputCols = oheOutputCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aa2e672-e30e-4c0b-b6cc-05dfc55a8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [\"TAXI_OUT\", \"DEP_DELAY\", \"CRS_ELAPSED_TIME\", \"DISTANCE\"]\n",
    "\n",
    "numVecAssembler = VectorAssembler(inputCols = num_features,\n",
    "                                  outputCol = 'features',\n",
    "                                  handleInvalid = 'skip')\n",
    "\n",
    "stdScaler = StandardScaler(inputCol = 'features',\n",
    "                           outputCol = 'features_scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf337d-24f4-403d-9d89-4c21c9055f27",
   "metadata": {},
   "source": [
    "## Assembling dos vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be58abec-5dec-470e-8add-fdb1d11f356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembleInputs = oheOutputCols + ['features_scaled']\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols = assembleInputs,\n",
    "                               outputCol = 'features_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66cc9e3b-ac35-4550-8d2d-722f2a6867cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [stringIndex, oheEncoder, numVecAssembler, stdScaler, vecAssembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f57bd8-ab90-45d9-9cd5-e0a3662d3a1c",
   "metadata": {},
   "source": [
    "## Criação do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6efea6d0-7063-4684-969f-33d36037ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>...</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>OP_CARRIERIndex</th>\n",
       "      <th>ORIGINIndex</th>\n",
       "      <th>DESTIndex</th>\n",
       "      <th>OP_CARRIEROHE</th>\n",
       "      <th>ORIGINOHE</th>\n",
       "      <th>DESTOHE</th>\n",
       "      <th>features</th>\n",
       "      <th>features_scaled</th>\n",
       "      <th>features_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2102</td>\n",
       "      <td>AUS</td>\n",
       "      <td>IND</td>\n",
       "      <td>1655</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>...</td>\n",
       "      <td>920.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[19.0, -5.0, 155.0, 920.0]</td>\n",
       "      <td>[1.7984320895620953, -0.1581283862793019, 2.22...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2103</td>\n",
       "      <td>IND</td>\n",
       "      <td>AUS</td>\n",
       "      <td>1445</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>...</td>\n",
       "      <td>920.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[14.0, 5.0, 150.0, 920.0]</td>\n",
       "      <td>[1.3251604870457545, 0.1581283862793019, 2.149...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2109</td>\n",
       "      <td>MSP</td>\n",
       "      <td>OKC</td>\n",
       "      <td>2130</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>695.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[34.0, -4.0, 133.0, 695.0]</td>\n",
       "      <td>[3.218246897111118, -0.12650270902344152, 1.90...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2110</td>\n",
       "      <td>OKC</td>\n",
       "      <td>MSP</td>\n",
       "      <td>1745</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>...</td>\n",
       "      <td>695.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[8.0, -5.0, 132.0, 695.0]</td>\n",
       "      <td>[0.7572345640261454, -0.1581283862793019, 1.89...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2114</td>\n",
       "      <td>ALO</td>\n",
       "      <td>MSP</td>\n",
       "      <td>700</td>\n",
       "      <td>815.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[16.0, 75.0, 69.0, 166.0]</td>\n",
       "      <td>[1.5144691280522908, 2.3719257941895284, 0.988...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2118</td>\n",
       "      <td>MSP</td>\n",
       "      <td>GRR</td>\n",
       "      <td>1310</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>...</td>\n",
       "      <td>408.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[14.0, 27.0, 93.0, 408.0]</td>\n",
       "      <td>[1.3251604870457545, 0.8538932859082302, 1.332...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2122</td>\n",
       "      <td>CLE</td>\n",
       "      <td>MSP</td>\n",
       "      <td>1343</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>...</td>\n",
       "      <td>622.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[11.0, -5.0, 135.0, 622.0]</td>\n",
       "      <td>[1.04119752553595, -0.1581283862793019, 1.9343...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2122</td>\n",
       "      <td>MSP</td>\n",
       "      <td>RHI</td>\n",
       "      <td>1538</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>...</td>\n",
       "      <td>190.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[12.0, 11.0, 58.0, 190.0]</td>\n",
       "      <td>[1.1358518460392182, 0.34788244981446415, 0.83...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2123</td>\n",
       "      <td>DTW</td>\n",
       "      <td>GRR</td>\n",
       "      <td>1218</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[16.0, -4.0, 62.0, 120.0]</td>\n",
       "      <td>[1.5144691280522908, -0.12650270902344152, 0.8...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>9E</td>\n",
       "      <td>2124</td>\n",
       "      <td>GRR</td>\n",
       "      <td>MSP</td>\n",
       "      <td>1350</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>...</td>\n",
       "      <td>408.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[9.0, -5.0, 100.0, 408.0]</td>\n",
       "      <td>[0.8518888845294136, -0.1581283862793019, 1.43...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE OP_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  CRS_DEP_TIME  \\\n",
       "0 2009-01-01         9E               2102    AUS  IND          1655   \n",
       "1 2009-01-01         9E               2103    IND  AUS          1445   \n",
       "2 2009-01-01         9E               2109    MSP  OKC          2130   \n",
       "3 2009-01-01         9E               2110    OKC  MSP          1745   \n",
       "4 2009-01-01         9E               2114    ALO  MSP           700   \n",
       "5 2009-01-01         9E               2118    MSP  GRR          1310   \n",
       "6 2009-01-01         9E               2122    CLE  MSP          1343   \n",
       "7 2009-01-01         9E               2122    MSP  RHI          1538   \n",
       "8 2009-01-01         9E               2123    DTW  GRR          1218   \n",
       "9 2009-01-01         9E               2124    GRR  MSP          1350   \n",
       "\n",
       "   DEP_TIME  DEP_DELAY  TAXI_OUT  WHEELS_OFF  ...  DISTANCE  OP_CARRIERIndex  \\\n",
       "0    1650.0       -5.0      19.0      1709.0  ...     920.0             10.0   \n",
       "1    1450.0        5.0      14.0      1504.0  ...     920.0             10.0   \n",
       "2    2126.0       -4.0      34.0      2200.0  ...     695.0             10.0   \n",
       "3    1740.0       -5.0       8.0      1748.0  ...     695.0             10.0   \n",
       "4     815.0       75.0      16.0       831.0  ...     166.0             10.0   \n",
       "5    1337.0       27.0      14.0      1351.0  ...     408.0             10.0   \n",
       "6    1338.0       -5.0      11.0      1349.0  ...     622.0             10.0   \n",
       "7    1549.0       11.0      12.0      1601.0  ...     190.0             10.0   \n",
       "8    1214.0       -4.0      16.0      1230.0  ...     120.0             10.0   \n",
       "9    1345.0       -5.0       9.0      1354.0  ...     408.0             10.0   \n",
       "\n",
       "   ORIGINIndex DESTIndex                                      OP_CARRIEROHE  \\\n",
       "0         44.0      45.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1         45.0      44.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2         12.0      59.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3         59.0      12.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4        276.0      12.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5         12.0      78.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6         32.0      12.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7         12.0     294.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8          8.0      78.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9         78.0      12.0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                           ORIGINOHE  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                             DESTOHE  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                     features  \\\n",
       "0  [19.0, -5.0, 155.0, 920.0]   \n",
       "1   [14.0, 5.0, 150.0, 920.0]   \n",
       "2  [34.0, -4.0, 133.0, 695.0]   \n",
       "3   [8.0, -5.0, 132.0, 695.0]   \n",
       "4   [16.0, 75.0, 69.0, 166.0]   \n",
       "5   [14.0, 27.0, 93.0, 408.0]   \n",
       "6  [11.0, -5.0, 135.0, 622.0]   \n",
       "7   [12.0, 11.0, 58.0, 190.0]   \n",
       "8   [16.0, -4.0, 62.0, 120.0]   \n",
       "9   [9.0, -5.0, 100.0, 408.0]   \n",
       "\n",
       "                                     features_scaled  \\\n",
       "0  [1.7984320895620953, -0.1581283862793019, 2.22...   \n",
       "1  [1.3251604870457545, 0.1581283862793019, 2.149...   \n",
       "2  [3.218246897111118, -0.12650270902344152, 1.90...   \n",
       "3  [0.7572345640261454, -0.1581283862793019, 1.89...   \n",
       "4  [1.5144691280522908, 2.3719257941895284, 0.988...   \n",
       "5  [1.3251604870457545, 0.8538932859082302, 1.332...   \n",
       "6  [1.04119752553595, -0.1581283862793019, 1.9343...   \n",
       "7  [1.1358518460392182, 0.34788244981446415, 0.83...   \n",
       "8  [1.5144691280522908, -0.12650270902344152, 0.8...   \n",
       "9  [0.8518888845294136, -0.1581283862793019, 1.43...   \n",
       "\n",
       "                                     features_vector  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criacao do pipeline de transformacao\n",
    "transform_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Aplicacao do pipeline nos dados de treino\n",
    "fitted_transformer = transform_pipeline.fit(train_df)\n",
    "transformed_train_df = fitted_transformer.transform(train_df)\n",
    "\n",
    "transformed_train_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b21d58-1fee-46ee-8d0c-ff0bda815b29",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b47ead3a-7b15-4e56-bba8-38be52c2875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(maxIter = 25, # pode causar overfitting\n",
    "                         solver = 'normal',\n",
    "                         labelCol = 'ARR_DELAY',\n",
    "                         featuresCol = 'features_vector',\n",
    "                         elasticNetParam = 0.2,\n",
    "                         regParam = 0.02)\n",
    "\n",
    "pipe_stages = stages + [model]\n",
    "\n",
    "pipe = Pipeline(stages=pipe_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de85147d-3792-4f20-899c-7d5617c20db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1712.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 166.0 failed 1 times, most recent failure: Lost task 0.0 in stage 166.0 (TID 383) (jupyter-caioag3 executor driver): scala.MatchError: [null,1.0,(612,[10,218,325,608,609,610,611],[1.0,1.0,1.0,2.2717036920784364,0.22137974079102266,2.1779428808522967,1.6244122287441223])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:451)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:345)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: scala.MatchError: [null,1.0,(612,[10,218,325,608,609,610,611],[1.0,1.0,1.0,2.2717036920784364,0.22137974079102266,2.1779428808522967,1.6244122287441223])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fitted_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1712.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 166.0 failed 1 times, most recent failure: Lost task 0.0 in stage 166.0 (TID 383) (jupyter-caioag3 executor driver): scala.MatchError: [null,1.0,(612,[10,218,325,608,609,610,611],[1.0,1.0,1.0,2.2717036920784364,0.22137974079102266,2.1779428808522967,1.6244122287441223])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:451)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:345)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: scala.MatchError: [null,1.0,(612,[10,218,325,608,609,610,611],[1.0,1.0,1.0,2.2717036920784364,0.22137974079102266,2.1779428808522967,1.6244122287441223])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "fitted_pipe = pipe.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5c16f-b2ea-4f98-9e33-bf7000f2412d",
   "metadata": {},
   "source": [
    "## Model performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da251e83-97f7-4a2e-8b3f-083c30e6483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fitted_pipe.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dc1e3-3c06-46c2-b33c-97d862aff1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
